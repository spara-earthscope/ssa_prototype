{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7596d84e-6f02-43af-93c3-44e4fb24d8bf",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "After acquiring the data the following step is to preprocess the data which increases the likelihood of finding similar events by applying filters (such as bandpass filters) , removing out-of-band noise, and allowing the desired signal's spectral characteristics to stand out.\n",
    "\n",
    "Pre-processing involves removing the mean value from a trace and centering it on zero. Another process is linear detrending, which forces the start and end of a trace to align to zero. This focuses on high-frequency events, such as P and S-waves from volcanic tectonic earthquakes. A third process is applying a bandpass filter which removes both low-frequency noise (greater than 5Hz for VT events) and high-frequency noise (lower than 25Hz for VT events).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ee597-406c-46be-99f9-7da1ee61b9c7",
   "metadata": {},
   "source": [
    "## Parallel Processing Patterns\n",
    "\n",
    "In the previouse exercise, we demonstrated how to use concurrency to efficiently download miniseed files. The pattern defined a single task (request and download data), create a function to manage multiple tasks, and call the function. We can use the same pattern tp perform computational tasks, such as waveform preprocessing, in parallel. Recall that parallel processing, unlike concurrency, launches multiple Python instances against CPU cores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1edd4-b015-4414-9029-189a196f8e9a",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce389f-9087-41be-8be7-e4cde19a5004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from obspy import read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e0515-0294-4216-8f20-07af9a762fe4",
   "metadata": {},
   "source": [
    "### Get a list of miniseed files in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f02720b-ceb3-4abc-83f7-9a979a8dff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_miniseed_files(input_dir, extensions=None):\n",
    "    \"\"\"\n",
    "    Get all miniseed files from the input directory.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dir : str or Path\n",
    "        Directory containing miniseed files\n",
    "    extensions : list of str, optional\n",
    "        File extensions to look for (default: ['.mseed', '.miniseed', '.ms'])\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list : List of Path objects for miniseed files\n",
    "    \"\"\"\n",
    "    if extensions is None:\n",
    "        extensions = ['.mseed', '.miniseed', '.ms']\n",
    "    \n",
    "    input_path = Path(input_dir)\n",
    "    \n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"Input directory not found: {input_dir}\")\n",
    "    \n",
    "    # Find all files with miniseed extensions\n",
    "    miniseed_files = []\n",
    "    for ext in extensions:\n",
    "        miniseed_files.extend(input_path.glob(f\"*{ext}\"))\n",
    "    \n",
    "    return sorted(miniseed_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7041ef77-eab1-4a48-a956-c0317f6d9127",
   "metadata": {},
   "source": [
    "> **Explainer:**\n",
    "> \n",
    "> This is a utility function that scans a directory and finds all files with miniseed extensions (.mseed, .miniseed, or .ms). It returns a sorted list of Path objects representing each miniseed file found. The function validates that the directory exists and raises an error if it doesn't, preventing silent failures later in the processing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfe30b8-df47-4385-9486-76b42dfc0957",
   "metadata": {},
   "source": [
    "### Preprocess a Single Miniseed File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04329561-6a3f-4ad8-9e15-9d941c077961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_single_file(input_filepath, output_dir, freqmin, freqmax, \n",
    "                           taper_percentage, corners, zerophase):\n",
    "    \"\"\"\n",
    "    Preprocess a single miniseed file with detrending, tapering, and filtering.\n",
    "    \n",
    "    Processing steps:\n",
    "    1. Linear detrend - removes linear trends in the data\n",
    "    2. Demean - removes the mean value (centers data around zero)\n",
    "    3. Taper - applies a window to reduce edge effects (default 5%)\n",
    "    4. Bandpass filter - keeps only frequencies within specified range\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_filepath : str or Path\n",
    "        Path to the input miniseed file\n",
    "    output_dir : str or Path\n",
    "        Directory to save the processed file\n",
    "    freqmin : float\n",
    "        Minimum frequency for bandpass filter (Hz)\n",
    "    freqmax : float\n",
    "        Maximum frequency for bandpass filter (Hz)\n",
    "    taper_percentage : float\n",
    "        Percentage of trace to taper (0-0.5)\n",
    "    corners : int\n",
    "        Number of corners for Butterworth filter\n",
    "    zerophase : bool\n",
    "        If True, apply zero-phase filter\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (success: bool, input_file: str, output_file: str or None, error: str or None)\n",
    "    \"\"\"\n",
    "    input_path = Path(input_filepath)\n",
    "    filename = input_path.name\n",
    "    \n",
    "    try:\n",
    "        # Read the miniseed file\n",
    "        st = read(str(input_filepath))\n",
    "        \n",
    "        # Get original file info for reporting\n",
    "        n_traces = len(st)\n",
    "        original_stats = f\"{n_traces} trace(s)\"\n",
    "        \n",
    "        # Process each trace in the stream\n",
    "        for tr in st:\n",
    "            # Step 1: Apply linear detrend (removes linear trend)\n",
    "            tr.detrend('linear')\n",
    "            \n",
    "            # Step 2: Remove mean (demean)\n",
    "            tr.detrend('demean')\n",
    "            \n",
    "            # Step 3: Apply taper (5% by default)\n",
    "            # Taper reduces edge effects in filtering\n",
    "            tr.taper(max_percentage=taper_percentage, type='hann')\n",
    "            \n",
    "            # Step 4: Apply bandpass filter\n",
    "            tr.filter('bandpass', \n",
    "                     freqmin=freqmin, \n",
    "                     freqmax=freqmax,\n",
    "                     corners=corners,\n",
    "                     zerophase=zerophase)\n",
    "        \n",
    "        # Create output filepath with \"_processed\" suffix\n",
    "        output_filename = input_path.stem + \"_processed\" + input_path.suffix\n",
    "        output_filepath = Path(output_dir) / output_filename\n",
    "        \n",
    "        # Save processed stream\n",
    "        st.write(str(output_filepath), format='MSEED')\n",
    "        \n",
    "        print(f\"✓ Processed: {filename} ({original_stats})\")\n",
    "        return (True, str(input_filepath), str(output_filepath), None)\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"{type(e).__name__}: {str(e)}\"\n",
    "        print(f\"✗ Failed: {filename} - {error_msg}\")\n",
    "        return (False, str(input_filepath), None, error_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b83702f-faea-4ce7-985d-33fe16eef68c",
   "metadata": {},
   "source": [
    "> **Explainer:**\n",
    "> \n",
    "> This is the core processing function that handles a single miniseed file. It reads the file using ObsPy, then applies four sequential processing steps to each trace:\n",
    "> 1. linear detrend to remove any long-term linear trends,\n",
    "> 2. demean to center the data around zero,\n",
    "> 3. a Hann taper applied to a small percentage (default 5%) of each end to prevent edge artifacts, and\n",
    "> 4.  a Butterworth bandpass filter to isolate frequencies of interest.\n",
    ">   \n",
    "> The processed data is saved with a \"_processed\" suffix and the function returns a tuple indicating success/failure along with file paths and any error messages. This function is designed to be called in parallel for multiple files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687f14fe-1064-43f2-91d1-1c5e4d596bd5",
   "metadata": {},
   "source": [
    "### Main Parallel Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abff8f0-5293-409f-92ab-0e4f6f9f0714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_miniseed_parallel(input_dir, output_dir=\"./processed_data\",\n",
    "                                 freqmin=0.1, freqmax=10.0,\n",
    "                                 taper_percentage=0.05, corners=4,\n",
    "                                 zerophase=True, max_workers=4):\n",
    "    \"\"\"\n",
    "    Preprocess multiple miniseed files in parallel.\n",
    "    \n",
    "    This function orchestrates the parallel processing of all miniseed files in a\n",
    "    directory. It uses ThreadPoolExecutor to process multiple files simultaneously,\n",
    "    which significantly reduces total processing time compared to sequential processing.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dir : str\n",
    "        Directory containing input miniseed files\n",
    "    output_dir : str, optional\n",
    "        Directory to save processed files (default: './processed_data')\n",
    "    freqmin : float, optional\n",
    "        Minimum frequency for bandpass filter in Hz (default: 0.1)\n",
    "    freqmax : float, optional\n",
    "        Maximum frequency for bandpass filter in Hz (default: 10.0)\n",
    "    taper_percentage : float, optional\n",
    "        Percentage of trace to taper, 0-0.5 (default: 0.05 = 5%)\n",
    "    corners : int, optional\n",
    "        Number of corners for Butterworth filter (default: 4)\n",
    "    zerophase : bool, optional\n",
    "        If True, apply zero-phase filter (default: True)\n",
    "    max_workers : int, optional\n",
    "        Maximum number of parallel workers (default: 4)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing 'successful' and 'failed' processing results\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> results = preprocess_miniseed_parallel(\n",
    "    ...     input_dir='./seismic_data',\n",
    "    ...     output_dir='./processed_data',\n",
    "    ...     freqmin=0.5,\n",
    "    ...     freqmax=20.0,\n",
    "    ...     max_workers=8\n",
    "    ... )\n",
    "    >>> print(f\"Processed {len(results['successful'])} files successfully\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all miniseed files from the input directory\n",
    "    miniseed_files = get_miniseed_files(input_dir)\n",
    "    \n",
    "    if not miniseed_files:\n",
    "        print(f\"No miniseed files found in {input_dir}\")\n",
    "        return {'successful': [], 'failed': []}\n",
    "    \n",
    "    # Print processing configuration\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Miniseed Preprocessing\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Input directory:  {input_dir}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(f\"Files found:      {len(miniseed_files)}\")\n",
    "    print(f\"Parallel workers: {max_workers}\")\n",
    "    print(f\"\\nProcessing parameters:\")\n",
    "    print(f\"  - Linear detrend:    enabled\")\n",
    "    print(f\"  - Demean:            enabled\")\n",
    "    print(f\"  - Taper:             {taper_percentage*100}% (Hann window)\")\n",
    "    print(f\"  - Bandpass filter:   {freqmin}-{freqmax} Hz\")\n",
    "    print(f\"  - Filter corners:    {corners}\")\n",
    "    print(f\"  - Zero-phase:        {zerophase}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {\n",
    "        'successful': [],\n",
    "        'failed': []\n",
    "    }\n",
    "    \n",
    "    # Process files in parallel using ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all processing tasks to the executor\n",
    "        # Each task is a Future object that will complete independently\n",
    "        future_to_file = {\n",
    "            executor.submit(\n",
    "                preprocess_single_file,\n",
    "                filepath, output_dir, freqmin, freqmax,\n",
    "                taper_percentage, corners, zerophase\n",
    "            ): filepath\n",
    "            for filepath in miniseed_files\n",
    "        }\n",
    "        \n",
    "        # Process results as they complete (not necessarily in submission order)\n",
    "        for future in as_completed(future_to_file):\n",
    "            filepath = future_to_file[future]\n",
    "            try:\n",
    "                # Get the result from the completed processing task\n",
    "                success, input_file, output_file, error = future.result()\n",
    "                \n",
    "                if success:\n",
    "                    # Add to successful processing list\n",
    "                    results['successful'].append({\n",
    "                        'input_file': input_file,\n",
    "                        'output_file': output_file\n",
    "                    })\n",
    "                else:\n",
    "                    # Add to failed processing list with error message\n",
    "                    results['failed'].append({\n",
    "                        'input_file': input_file,\n",
    "                        'error': error\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                # Handle any unexpected errors\n",
    "                error_msg = f\"Unexpected error: {str(e)}\"\n",
    "                print(f\"✗ Failed: {filepath.name} - {error_msg}\")\n",
    "                results['failed'].append({\n",
    "                    'input_file': str(filepath),\n",
    "                    'error': error_msg\n",
    "                })\n",
    "    \n",
    "    # Print summary of processing results\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing Summary:\")\n",
    "    print(f\"  ✓ Successful: {len(results['successful'])} files\")\n",
    "    print(f\"  ✗ Failed:     {len(results['failed'])} files\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # List failed files if any\n",
    "    if results['failed']:\n",
    "        print(f\"\\nFailed files:\")\n",
    "        for item in results['failed']:\n",
    "            print(f\"  - {Path(item['input_file']).name}: {item['error']}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bbcb82-f53d-4739-8be6-938f03459ae4",
   "metadata": {},
   "source": [
    "> **Explainer:**\n",
    "> \n",
    "> This is the main orchestration function that manages the parallel processing workflow. It first creates the output directory and finds all miniseed files in the input directory, then displays a detailed configuration summary.\n",
    ">\n",
    "> The function uses ThreadPoolExecutor to create a pool of worker threads (specified by max_workers) and submits all files for processing simultaneously. As each file completes processing (tracked via as_completed()), the results are collected into 'successful' and 'failed' lists.\n",
    ">\n",
    "> This parallel approach can dramatically reduce processing time - for example, processing 20 files with 4 workers could be ~4x faster than sequential processing.\n",
    ">\n",
    "> The function returns a dictionary with detailed results for each file, making it easy to track what succeeded and what failed, and includes comprehensive error reporting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81663cf-6da2-4d85-965d-6bf12ec51aaa",
   "metadata": {},
   "source": [
    "### Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d54a25-bb6b-44f5-8015-4887b4bec234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic usage with default parameters\n",
    "# -----------------------------------------------\n",
    "# Process all miniseed files in './seismic_data' with default filter settings\n",
    "\n",
    "results = preprocess_miniseed_parallel(\n",
    "    input_dir='./seismic_data',\n",
    "    output_dir='./processed_data'\n",
    ")\n",
    "\n",
    "# Example 2: Custom filter parameters\n",
    "# ------------------------------------\n",
    "# Process files with a higher frequency range (0.5-20 Hz) and more workers\n",
    "\n",
    "results = preprocess_miniseed_parallel(\n",
    "    input_dir='./seismic_data',\n",
    "    output_dir='./processed_data',\n",
    "    freqmin=0.5,        # Higher low-frequency cutoff\n",
    "    freqmax=20.0,       # Higher high-frequency cutoff\n",
    "    taper_percentage=0.1,   # 10% taper instead of 5%\n",
    "    max_workers=8       # Use 8 parallel workers\n",
    ")\n",
    "\n",
    "# Example 3: Access and analyze results\n",
    "# --------------------------------------\n",
    "\n",
    "print(f\"\\nSuccessfully processed files:\")\n",
    "for item in results['successful']:\n",
    "    print(f\"  Input:  {item['input_file']}\")\n",
    "    print(f\"  Output: {item['output_file']}\")\n",
    "    print()\n",
    "\n",
    "if results['failed']:\n",
    "    print(f\"\\nFailed files (need attention):\")\n",
    "    for item in results['failed']:\n",
    "        print(f\"  File:  {item['input_file']}\")\n",
    "        print(f\"  Error: {item['error']}\")\n",
    "        print()\n",
    "\n",
    "# Example 4: Save results to a CSV file for record-keeping\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convert results to DataFrames\n",
    "if results['successful']:\n",
    "    successful_df = pd.DataFrame(results['successful'])\n",
    "    successful_df.to_csv('successful_processing.csv', index=False)\n",
    "    print(f\"Saved successful results to: successful_processing.csv\")\n",
    "\n",
    "if results['failed']:\n",
    "    failed_df = pd.DataFrame(results['failed'])\n",
    "    failed_df.to_csv('failed_processing.csv', index=False)\n",
    "    print(f\"Saved failed results to: failed_processing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6601b45-0967-453f-b35f-212d9f488d4f",
   "metadata": {},
   "source": [
    "> **Explainer:**\n",
    ">\n",
    ">   Example 1 shows the simplest usage with default parameters suitable for most seismic data processing tasks (0.1-10 Hz is good for teleseismic and regional events).\n",
    ">\n",
    "> Example 2 demonstrates how to customize the filter parameters for specific applications - higher frequencies (0.5-20 Hz) are better for local events and crustal studies, while the increased worker count speeds up processing of large datasets.\n",
    ">\n",
    "> Example 3 shows how to access and iterate through the results dictionary to examine what was processed and what failed.\n",
    ">\n",
    "> Example 4 demonstrates saving results to CSV files for documentation and quality control purposes, which is useful for tracking processing workflows and debugging issues with specific files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
