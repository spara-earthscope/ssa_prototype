{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f92851-89e6-4ebe-ad19-1a8f7b0d0666",
   "metadata": {},
   "source": [
    "# Data Access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb06d04-945b-4109-8820-5fe361629a70",
   "metadata": {},
   "source": [
    "## Getting Data via Serial Download\n",
    "\n",
    "This tutorial demonstrates a common method of acquiring data that is useful for data exploration. This method involves the following:\n",
    "\n",
    "1. Download one or several miniseed files from a data provider. We will use EarthScope's FDSN service to request files.\n",
    "2. Read each stream extract metadata.\n",
    "3. Process the data by removing trends (linear, mean, taper) and applying a bandpass filter. This process normalizes the data for comparison.\n",
    "4. Visualize the data pre and post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac0317-7b79-4eba-9843-275682e14596",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We will use built-in python packages and obspy. These packages are already included in GeoLab; you will not need to install them. We start the script by importing the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede254e-7bad-4d38-b0b0-f20802298bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from obspy import UTCDateTime, read as obspy_read\n",
    "from obspy.clients.fdsn import Client, URL_MAPPINGS\n",
    "from obspy.clients.fdsn.header import FDSNNoDataException\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c576b33-93b2-4691-8f1c-80f203b93e86",
   "metadata": {},
   "source": [
    "### IMUSH Data\n",
    "\n",
    "A listing for miniseed data is available for Mt. St. Helens from IMUSH (\n",
    "Imaging Magma Under St. Helens). The [web page](https://ds.iris.edu/mda/XD/?starttime=2014-01-01T00%3A00%3A00&endtime=2016-12-31T23%3A59%3A59#XD_2014-01-01_2016-12-31) lists the stations that recorded activity from 2014 to 2016.\n",
    "\n",
    "The stations that have miniseed data have been saved in the IMUSH.csv file.\n",
    "\n",
    "**Protip**\n",
    "> The listing is dynamically generated by JavaScript, which makes scraping the stations we want more complicated. A simple solution is to copy the stations of interest and paste them into a spreadsheet such Google Sheets and save it as a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f7368-bd9c-49a6-8639-b33462552240",
   "metadata": {},
   "source": [
    "### Getting Stations Data\n",
    "\n",
    "The IMUSH.csv provides the station names and the start and end times for the recorded data. We can use this information to request the data by reading the CSV file. When we read each row of the CSV file, we need to store the data using a `@dataclass`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94fda37-158f-4315-84fc-c903c2926f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class StationRow:\n",
    "    station: str\n",
    "    datacenter: str\n",
    "    start: UTCDateTime\n",
    "    end: UTCDateTime\n",
    "    site: str\n",
    "    latitude: float\n",
    "    longitude: float\n",
    "    elevation_m: float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bc1338-760a-4ff3-adc1-c82915ff1402",
   "metadata": {},
   "source": [
    "We will need a function to read the CSV file and put them in a list. The function has three parameters, the first is the path and name of the file, a start date, and an end date. The function uses the `pandas` package to read the file. Pandas treats the rows of the data as a table so we can select all the rows or a specific set.\n",
    "\n",
    "Note that the function uses the `StationRow` data class and returns a Python list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a0804d-110c-491a-9003-c1172f5b37c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(csv_path: str | Path, *, start_row: Optional[int] = None, end_row: Optional[int] = None) -> List[StationRow]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Read a station CSV using pandas and return StationRow dataclass objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_path : str or Path\n",
    "        Path to the CSV file.\n",
    "\n",
    "    start_row : int, optional\n",
    "        Zero-based index of the first row to read (inclusive).\n",
    "        If None, starts from the beginning.\n",
    "\n",
    "    end_row : int, optional\n",
    "        Zero-based index of the last row to read (exclusive).\n",
    "        If None, reads through the end of the file.\n",
    "\n",
    "    Behavior\n",
    "    --------\n",
    "    - If start_row and end_row are both None, all rows are read.\n",
    "    - Rows are selected using df.iloc[start_row:end_row].\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Slice rows (pandas handles None cleanly)\n",
    "    df_sel = df.iloc[start_row:end_row]\n",
    "\n",
    "    station_rows: List[StationRow] = []\n",
    "\n",
    "    for _, r in df_sel.iterrows():\n",
    "        station_rows.append(\n",
    "            StationRow(\n",
    "                station=str(r[\"Station\"]).strip(),\n",
    "                datacenter=str(r[\"DataCenter\"]).strip(),\n",
    "                start=UTCDateTime(str(r[\"Start\"])),\n",
    "                end=UTCDateTime(str(r[\"End\"])) + 86400,  # inclusive end date\n",
    "                site=str(r[\"Site\"]).strip(),\n",
    "                latitude=float(r[\"Latitude\"]),\n",
    "                longitude=float(r[\"Longitude\"]),\n",
    "                elevation_m=float(r[\"Elevation\"]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return station_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cfae06-d354-40e1-bd26-d3861ac665fc",
   "metadata": {},
   "source": [
    "Let's try out the `read_csv` function and print out six rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a5fbc-004b-4298-b43a-baa7fa32bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = read_csv(\"IMUSH.csv\", start_row=0, end_row=5)\n",
    "\n",
    "for s in stations:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21f9772-c1ee-4ca5-bf14-05c46dbb3281",
   "metadata": {},
   "source": [
    "Note that the index of the first row of a pandas table or dataframe starts at 0. Note the station in the first row. Change the `start_row` parameter to 0 and compare the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39b48f3-655a-4d03-9a2e-a27ceeb2c407",
   "metadata": {},
   "source": [
    "### Downloading Miniseed files\n",
    "\n",
    "Next we will download three miniseed files and save them in GeoLab. Using `obspy` we'll write a function to download files by a list we provide from the IMUSH.csv file. We can use `obspy` to request the data from EarthScope's FDSN service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8429719e-9f98-421a-9a82-783ea1d3bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_miniseed(station_rows, *, starttime=None, endtime=None, output_dir=\"./seismic_data\"):\n",
    "    \"\"\"\n",
    "    Download miniseed file from EarthScope's FDSN service.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    station_rows : str\n",
    "        Station code (e.g., 'ANMO')\n",
    "    start_date : str\n",
    "        Start date in format 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SS'\n",
    "    end_date : str\n",
    "        End date in format 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SS'\n",
    "    output_dir : str, optional\n",
    "        Directory to save the miniseed file (default: './seismic_data')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str : Path to the saved miniseed file\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> download_miniseed('ANMO', 'IU', '2024-01-01', '2024-01-02')\n",
    "    \"\"\"\n",
    "    \n",
    "    # default values\n",
    "    network = \"XD\"\n",
    "    location = \"*\"\n",
    "    channel = \"*HZ\"\n",
    "    \n",
    "    # parse list as a tuple using list comprehension\n",
    "    station_data = [(row.station, row.start, row.end) for row in station_rows]\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize EarthScope FDSN client\n",
    "    client = Client('IRIS')  # IRIS is part of EarthScope\n",
    "    \n",
    "    # Download waveform data\n",
    "    for station, start, end in station_data:\n",
    "        actual_start = starttime if starttime is not None else start\n",
    "        actual_end = endtime if endtime is not None else end\n",
    "        starttime=UTCDateTime(actual_start)\n",
    "        endtime=UTCDateTime(actual_end)\n",
    "        \n",
    "        try:\n",
    "            st = client.get_waveforms(\n",
    "                network=network,\n",
    "                station=station,\n",
    "                location=location,\n",
    "                channel=channel,\n",
    "                starttime=starttime,\n",
    "                endtime=endtime\n",
    "            )\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "        # Create filename\n",
    "        filename = f\"{network}_{station}_{starttime.strftime('%Y%m%d')}_{endtime.strftime('%Y%m%d')}.mseed\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Save to miniseed file\n",
    "        st.write(filepath, format='MSEED')\n",
    "        print(f\"Successfully saved to: {filepath}\")\n",
    "        \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40b2a1c-9284-41b1-b516-12fbebf5dafc",
   "metadata": {},
   "source": [
    "This is how to call the function. Note that the optionl start and end times are specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b703c-30de-4d1a-807f-5630e1f57c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime=UTCDateTime(2014, 7, 17)\n",
    "endtime=UTCDateTime(2014, 7, 18)\n",
    "\n",
    "stations = read_csv(\"IMUSH.csv\", start_row=5, end_row=8)\n",
    "\n",
    "filepath = download_miniseed(stations, starttime=starttime, endtime=endtime)\n",
    "                             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7834fe99-2d36-4ea2-be19-7b975382fe34",
   "metadata": {},
   "source": [
    "Try downloading a single station without specifying the start and end time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71d42c-02b2-41ed-a761-d599578bdc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a single station without a start and end time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8e7e5-4624-4adc-b7b7-3cb375f18057",
   "metadata": {},
   "source": [
    "## Parallel Seismic Data Download from EarthScope FDSN\n",
    "\n",
    "This notebook demonstrates how to download seismic miniseed files in parallel from EarthScope's FDSN service using Python's concurrent.futures module.\n",
    "\n",
    "### Concurrency and Parallel Processing with Python\n",
    "\n",
    "Concurrency and parallel processing in Python allow programs to handle multiple tasks more efficiently, though they work in different ways. Python has a Global Interpreter Lock (GIL) that prevents multiple threads from executing Python bytecode simultaneously, which means threads don't truly run in parallel for CPU-bound tasks.\n",
    "\n",
    "Concurrency involves managing multiple tasks without necessarily running simultaneously. Python juggles multiple tasks by switching between them rapidly. When one task is waiting (like waiting for a file to download or a database to respond), Python can pause that task and work on something else productive instead of just sitting idle.\n",
    "\n",
    "Parallel processing, on the other hand, actually executes multiple computations simultaneously across multiple CPU cores. Parallel processing works by launching multiple instances of Python as separate processes across cores, making it ideal for CPU-intensive tasks like data processing or mathematical computations. The key distinction is that concurrency is about dealing with multiple things at once by switching between them efficiently, while parallelism is about doing multiple things at the exact same time.\n",
    "\n",
    "### Import Required Libraries\n",
    "\n",
    "We'll need ObsPy for seismic data handling and `concurrent.futures` for parallel downloads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452ee24f-422a-432c-85ce-b27c79b6620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c52268-270c-4053-8f4e-ef4b0b3ced8c",
   "metadata": {},
   "source": [
    "### Single Station Download Function\n",
    "\n",
    "This function (or task) handles downloading data for one station at a time. It will be called in parallel by the `download_miniseed` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca174eb-285d-44f6-86de-48a967d0c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_single_station(station, start, end, network, location, channel, \n",
    "                            starttime_override, endtime_override, output_dir, client):\n",
    "    \"\"\"\n",
    "    Download miniseed file for a single station.\n",
    "    \n",
    "    This function is designed to be called in parallel for multiple stations.\n",
    "    It handles all the logic for one download operation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    station : str\n",
    "        Station code (e.g., 'ANMO')\n",
    "    start : str\n",
    "        Station's start date from metadata\n",
    "    end : str\n",
    "        Station's end date from metadata\n",
    "    network : str\n",
    "        Network code (e.g., 'XD')\n",
    "    location : str\n",
    "        Location code (e.g., '*' for all locations)\n",
    "    channel : str\n",
    "        Channel code (e.g., 'BH?' for all BH channels)\n",
    "    starttime_override : str or None\n",
    "        Override start time if provided\n",
    "    endtime_override : str or None\n",
    "        Override end time if provided\n",
    "    output_dir : str\n",
    "        Directory to save the file\n",
    "    client : obspy.clients.fdsn.Client\n",
    "        FDSN client instance\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (success: bool, filepath: str or None, error: str or None)\n",
    "    \"\"\"\n",
    "    # Determine actual start/end times (use override if provided, otherwise use station metadata)\n",
    "    actual_start = starttime_override if starttime_override is not None else start\n",
    "    actual_end = endtime_override if endtime_override is not None else end\n",
    "    starttime = UTCDateTime(actual_start)\n",
    "    endtime = UTCDateTime(actual_end)\n",
    "    \n",
    "    try:\n",
    "        # Request waveform data from the FDSN service\n",
    "        st = client.get_waveforms(\n",
    "            network=network,\n",
    "            station=station,\n",
    "            location=location,\n",
    "            channel=channel,\n",
    "            starttime=starttime,\n",
    "            endtime=endtime\n",
    "        )\n",
    "        \n",
    "        # Create a descriptive filename with network, station, and date range\n",
    "        filename = f\"{network}_{station}_{starttime.strftime('%Y%m%d')}_{endtime.strftime('%Y%m%d')}.mseed\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Save the waveform data to a miniseed file\n",
    "        st.write(filepath, format='MSEED')\n",
    "        print(f\"✓ Successfully saved to: {filepath}\")\n",
    "        \n",
    "        return (True, filepath, None)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # If download fails, print error and return failure status\n",
    "        print(f\"✗ Failed to download {station}: {str(e)}\")\n",
    "        return (False, None, str(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0897c2-977b-4349-9808-2a819eefd1fa",
   "metadata": {},
   "source": [
    "> **Explainer:**\n",
    ">\n",
    "> Note the similarity between this code and the previous miniseed download example. Instead of reading the function parameters from a CSV file, each parameter required by the FDSN dataselect service is specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a99ec59-74ab-497d-ac6b-f0650759de8e",
   "metadata": {},
   "source": [
    "### Parallel Download Function\n",
    "\n",
    "This is the function that orchestrates parallel downloads for multiple stations by reading a list of stations, start times, and end times. The list can be constructed programmatically or read from a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820789db-6ae9-4513-9278-6d00e8c41aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_miniseed(station_rows, *, starttime=None, endtime=None, \n",
    "                     output_dir=\"./seismic_data\", max_workers=5):\n",
    "    \"\"\"\n",
    "    Download miniseed files from EarthScope's FDSN service in parallel.\n",
    "    \n",
    "    This function uses ThreadPoolExecutor to download data from multiple stations\n",
    "    simultaneously, significantly reducing total download time compared to sequential\n",
    "    downloads.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    station_rows : iterable\n",
    "        Iterable of station objects with .station, .start, .end attributes\n",
    "        (e.g., rows from a pandas DataFrame or list of namedtuples)\n",
    "    starttime : str, optional\n",
    "        Override start time in format 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SS'\n",
    "        If None, uses each station's individual start time\n",
    "    endtime : str, optional\n",
    "        Override end time in format 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SS'\n",
    "        If None, uses each station's individual end time\n",
    "    output_dir : str, optional\n",
    "        Directory to save the miniseed files (default: './seismic_data')\n",
    "    max_workers : int, optional\n",
    "        Maximum number of parallel downloads (default: 5)\n",
    "        Increase for faster downloads, but be mindful of server limits\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing:\n",
    "        - 'successful': List of dicts with station names and filepaths\n",
    "        - 'failed': List of dicts with station names and error messages\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> # Assuming you have a DataFrame with station information\n",
    "    >>> import pandas as pd\n",
    "    >>> stations_df = pd.DataFrame({\n",
    "    ...     'station': ['ANMO', 'CCM', 'HLID'],\n",
    "    ...     'start': ['2024-01-01', '2024-01-01', '2024-01-01'],\n",
    "    ...     'end': ['2024-01-02', '2024-01-02', '2024-01-02']\n",
    "    ... })\n",
    "    >>> \n",
    "    >>> # Download data for all stations in parallel\n",
    "    >>> results = download_miniseed(\n",
    "    ...     stations_df.itertuples(),\n",
    "    ...     starttime='2024-01-01',\n",
    "    ...     endtime='2024-01-02',\n",
    "    ...     max_workers=10\n",
    "    ... )\n",
    "    >>> \n",
    "    >>> print(f\"Downloaded {len(results['successful'])} files\")\n",
    "    >>> print(f\"Failed: {len(results['failed'])} files\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default FDSN parameters for EarthScope/IRIS network\n",
    "    network = \"XD\"        # Network code\n",
    "    location = \"*\"        # All locations\n",
    "    channel = \"*HZ\"       # All broadband high-gain channels (BHZ, BHN, BHE)\n",
    "    \n",
    "    # Extract station data from the input rows\n",
    "    # This creates a list of tuples: [(station_code, start_date, end_date), ...]\n",
    "    station_data = [(row.station, row.start, row.end) for row in station_rows]\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    # Initialize EarthScope FDSN client\n",
    "    # The client object is thread-safe and can be shared across threads\n",
    "    client = Client('IRIS')\n",
    "    print(f\"Initialized IRIS/EarthScope client\")\n",
    "    print(f\"Preparing to download {len(station_data)} stations with {max_workers} parallel workers\\n\")\n",
    "    \n",
    "    # Initialize results dictionary to track successful and failed downloads\n",
    "    results = {\n",
    "        'successful': [],\n",
    "        'failed': []\n",
    "    }\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel downloads\n",
    "    # ThreadPoolExecutor is ideal for I/O-bound tasks like network downloads\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all download tasks to the executor\n",
    "        # This creates a Future object for each station download\n",
    "        future_to_station = {\n",
    "            executor.submit(\n",
    "                download_single_station,\n",
    "                station, start, end, network, location, channel,\n",
    "                starttime, endtime, output_dir, client\n",
    "            ): station\n",
    "            for station, start, end in station_data\n",
    "        }\n",
    "        \n",
    "        # Process completed downloads as they finish (not in submission order)\n",
    "        # as_completed() yields futures as they complete, allowing real-time progress updates\n",
    "        for future in as_completed(future_to_station):\n",
    "            station = future_to_station[future]\n",
    "            try:\n",
    "                # Get the result from the completed future\n",
    "                success, filepath, error = future.result()\n",
    "                \n",
    "                if success:\n",
    "                    # Add to successful downloads list\n",
    "                    results['successful'].append({\n",
    "                        'station': station,\n",
    "                        'filepath': filepath\n",
    "                    })\n",
    "                else:\n",
    "                    # Add to failed downloads list with error message\n",
    "                    results['failed'].append({\n",
    "                        'station': station,\n",
    "                        'error': error\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                # Handle any unexpected errors that weren't caught in download_single_station\n",
    "                print(f\"✗ Unexpected error for station {station}: {str(e)}\")\n",
    "                results['failed'].append({\n",
    "                    'station': station,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "    \n",
    "    # Print summary of download results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Download Summary:\")\n",
    "    print(f\"  ✓ Successful: {len(results['successful'])} stations\")\n",
    "    print(f\"  ✗ Failed: {len(results['failed'])} stations\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0581c9f3-abbd-4a69-854e-f47670f351f8",
   "metadata": {},
   "source": [
    "> **Explainer:**\n",
    ">\n",
    "> This code uses Python's `ThreadPoolExecutor` to download seismic data from multiple stations simultaneously by creating a pool of worker threads (controlled by max_workers) that can execute download tasks concurrently. \n",
    ">\n",
    "> First, it submits all download tasks to the executor using `executor.submit()`, which immediately returns a Future object for each station. Future objects act as placeholders for results that will arrive later, and they're stored in a dictionary (future_to_station) that maps each Future to its corresponding station name for tracking purposes. \n",
    ">\n",
    "> Instead of waiting for all downloads to finish, the code `uses as_completed()` to process results as soon as each individual download completes (which may be in a different order than they were submitted), allowing for real-time progress updates and immediate handling of both successful downloads (storing the filepath) and failures (storing the error message). \n",
    ">\n",
    "> The with statement ensures that all threads are properly cleaned up when the downloads finish, and the try-except block catches any unexpected errors that might occur when retrieving results from the Future objects. The entire process is robust and efficient for I/O-bound network operations where threads spend most of their time waiting for server responses rather than consuming CPU resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6af0f5a-79c8-4398-b110-35b74dc0def0",
   "metadata": {},
   "source": [
    "### Example Usage\n",
    "\n",
    "Below are examples demonstrating the parallel download function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829f336f-0dc1-4d89-b759-4f0b93577909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Download data for multiple stations\n",
    "# -----------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create sample station data\n",
    "stations_df = pd.DataFrame({\n",
    "    'station': ['ANMO', 'CCM', 'HLID', 'SRU'],\n",
    "    'start': ['2024-01-01', '2024-01-01', '2024-01-01', '2024-01-01'],\n",
    "    'end': ['2024-01-02', '2024-01-02', '2024-01-02', '2024-01-02']\n",
    "})\n",
    "\n",
    "# Download with 10 parallel workers\n",
    "results = download_miniseed(\n",
    "    stations_df.itertuples(),\n",
    "    starttime='2014-07-17T00:00:00', # optional override\n",
    "    endtime='2014-07-19T00:00:00', # optional override\n",
    "    output_dir='./my_seismic_data', # optional override\n",
    "    max_workers=10\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nSuccessful downloads:\")\n",
    "for item in results['successful']:\n",
    "    print(f\"  {item['station']}: {item['filepath']}\")\n",
    "\n",
    "if results['failed']:\n",
    "    print(\"\\nFailed downloads:\")\n",
    "    for item in results['failed']:\n",
    "        print(f\"  {item['station']}: {item['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0afaee-b75d-4770-b3b2-dff7a454bb50",
   "metadata": {},
   "source": [
    "> **Explainer:**\n",
    ">\n",
    "> This examples uses a pandas dataframe to create table that specifies a station with a start and end time. It calls the `download_miniseed` function to download the files in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3ab1b1-29b3-4e28-939e-32c19b69e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Download the iMUSH data using the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a228e1fc-b1f8-43be-9e55-a877aed64ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Using with a larger station list from a CSV file\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read station list from CSV\n",
    "stations_df = pd.read_csv('stations.csv')\n",
    "\n",
    "# Download data with higher parallelism for faster processing\n",
    "results = download_miniseed(\n",
    "    stations_df.itertuples(),\n",
    "    starttime='2014-07-17',\n",
    "    endtime='2014-07-19',  # One day of data\n",
    "    output_dir='./daily_seismic_data',\n",
    "    max_workers=20  # Process 20 stations simultaneously\n",
    ")\n",
    "\n",
    "# Save results to CSV for record keeping\n",
    "import pandas as pd\n",
    "successful_df = pd.DataFrame(results['successful'])\n",
    "failed_df = pd.DataFrame(results['failed'])\n",
    "\n",
    "successful_df.to_csv('successful_downloads.csv', index=False)\n",
    "if len(failed_df) > 0:\n",
    "    failed_df.to_csv('failed_downloads.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e535c4dd-55d7-41e3-b704-c779c15b8cd8",
   "metadata": {},
   "source": [
    "> **Explainer:**\n",
    ">\n",
    "> Instead of creating a table of stations with start and endtime programmatically, we can read the data from a CSV file and use 20 workers to down load the files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e422e4c-d430-40f2-b7c5-288567f7dafa",
   "metadata": {},
   "source": [
    "### Performance Comparison\n",
    "The parallel approach can dramatically reduce download time:\n",
    "\n",
    "Sequential (original code):\n",
    "  - 10 stations × 5 seconds each = 50 seconds total\n",
    "\n",
    "Parallel (with max_workers=10):\n",
    "  - 10 stations ÷ 5 workers × 5 seconds = 10 seconds total\n",
    "  - ~5x speedup!\n",
    "\n",
    "Parallel (with max_workers=20):\n",
    "  - 10 stations ÷ 10 workers × 5 seconds = 5 seconds total\n",
    "  - ~10x speedup!\n",
    "\n",
    "Note: Actual speedup depends on network bandwidth and server response times."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
