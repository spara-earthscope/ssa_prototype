{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "674e8b06-bcb4-4cd9-86e1-32c0af41c756",
   "metadata": {},
   "source": [
    "# Getting Data via Serial Download\n",
    "\n",
    "This tutorial demonstrates a common method of acquiring data that is useful for data exploration. This method involves the following:\n",
    "\n",
    "1. Download one or several miniseed files from a data provider. We will use EarthScope's FDSN service to request files.\n",
    "2. Read each stream extract metadata.\n",
    "3. Process the data by removing trends (linear, mean, taper) and applying a bandpass filter. This process normalizes the data for comparison.\n",
    "4. Visualize the data pre and post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65962a85-d2b9-4f7f-8b2e-dc3d40c3d5f3",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We will use built-in python packages and obspy. These packages are already included in GeoLab; you will not need to install them. We start the script by importing the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "150eef13-deb3-4b4f-85f2-60e81b193467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from obspy import UTCDateTime, read as obspy_read\n",
    "from obspy.clients.fdsn import Client, URL_MAPPINGS\n",
    "from obspy.clients.fdsn.header import FDSNNoDataException\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0796c44-2905-46af-ab45-eceae1f27271",
   "metadata": {},
   "source": [
    "## IMUSH Data\n",
    "\n",
    "A listing for miniseed data is available for Mt. St. Helens from IMUSH (\n",
    "Imaging Magma Under St. Helens). The [web page](https://ds.iris.edu/mda/XD/?starttime=2014-01-01T00%3A00%3A00&endtime=2016-12-31T23%3A59%3A59#XD_2014-01-01_2016-12-31) lists the stations that recorded activity from 2014 to 2016.\n",
    "\n",
    "The stations that have miniseed data have been saved in the IMUSH.csv file.\n",
    "\n",
    "**Protip**\n",
    "> The listing is dynamically generated by JavaScript, which makes scraping the stations we want more complicated. A simple solution is to copy the stations of interest and paste them into a spreadsheet such Google Sheets and save it as a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8969d0a-15c3-440c-9edc-82b25fae660d",
   "metadata": {},
   "source": [
    "## Getting Stations Data\n",
    "\n",
    "The IMUSH.csv provides the station names and the start and end times for the recorded data. We can use this information to request the data by reading the CSV file. When we read each row of the CSV file, we need to store the data using a `@dataclass`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54410028-bdf3-4925-bca5-c74652874ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class StationRow:\n",
    "    station: str\n",
    "    datacenter: str\n",
    "    start: UTCDateTime\n",
    "    end: UTCDateTime\n",
    "    site: str\n",
    "    latitude: float\n",
    "    longitude: float\n",
    "    elevation_m: float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5e7528-0969-4ba4-87b8-c44b515104b2",
   "metadata": {},
   "source": [
    "We will need a function to read the CSV file and put them in a list. The function has three parameters, the first is the path and name of the file, a start date, and an end date. The function uses the `pandas` package to read the file. Pandas treats the rows of the data as a table so we can select all the rows or a specific set.\n",
    "\n",
    "Note that the function uses the `StationRow` data class and returns a Python list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1478706e-be50-46a3-bafc-4cba95b23f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(\n",
    "    csv_path: str | Path,\n",
    "    *,\n",
    "    start_row: Optional[int] = None,\n",
    "    end_row: Optional[int] = None,\n",
    ") -> List[StationRow]:\n",
    "    \"\"\"\n",
    "    Read a station CSV using pandas and return StationRow dataclass objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_path : str or Path\n",
    "        Path to the CSV file.\n",
    "\n",
    "    start_row : int, optional\n",
    "        Zero-based index of the first row to read (inclusive).\n",
    "        If None, starts from the beginning.\n",
    "\n",
    "    end_row : int, optional\n",
    "        Zero-based index of the last row to read (exclusive).\n",
    "        If None, reads through the end of the file.\n",
    "\n",
    "    Behavior\n",
    "    --------\n",
    "    - If start_row and end_row are both None, all rows are read.\n",
    "    - Rows are selected using df.iloc[start_row:end_row].\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Slice rows (pandas handles None cleanly)\n",
    "    df_sel = df.iloc[start_row:end_row]\n",
    "\n",
    "    station_rows: List[StationRow] = []\n",
    "\n",
    "    for _, r in df_sel.iterrows():\n",
    "        station_rows.append(\n",
    "            StationRow(\n",
    "                station=str(r[\"Station\"]).strip(),\n",
    "                datacenter=str(r[\"DataCenter\"]).strip(),\n",
    "                start=UTCDateTime(str(r[\"Start\"])),\n",
    "                end=UTCDateTime(str(r[\"End\"])) + 86400,  # inclusive end date\n",
    "                site=str(r[\"Site\"]).strip(),\n",
    "                latitude=float(r[\"Latitude\"]),\n",
    "                longitude=float(r[\"Longitude\"]),\n",
    "                elevation_m=float(r[\"Elevation\"]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return station_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee0debe-e23a-421b-a452-ae67515df8e8",
   "metadata": {},
   "source": [
    "Let's try out the `read_csv` function and print out the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0fca843-4193-437b-bf18-5303e6a9eb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StationRow(station='KRES', datacenter='IRISDMC', start=UTCDateTime(2014, 5, 1, 0, 0), end=UTCDateTime(2017, 1, 1, 0, 0), site='KRES', latitude=47.758739, longitude=-122.29097, elevation_m=52.0)\n",
      "StationRow(station='MA05', datacenter='IRISDMC', start=UTCDateTime(2014, 5, 1, 0, 0), end=UTCDateTime(2017, 1, 1, 0, 0), site='MA05', latitude=46.754669, longitude=-122.226189, elevation_m=488.0)\n",
      "StationRow(station='MB05', datacenter='IRISDMC', start=UTCDateTime(2014, 5, 1, 0, 0), end=UTCDateTime(2017, 1, 1, 0, 0), site='MB05', latitude=46.620869, longitude=-122.281021, elevation_m=641.0)\n",
      "StationRow(station='MB07', datacenter='IRISDMC', start=UTCDateTime(2014, 5, 1, 0, 0), end=UTCDateTime(2017, 1, 1, 0, 0), site='MB07', latitude=46.623779, longitude=-122.042389, elevation_m=878.0)\n",
      "StationRow(station='MC06', datacenter='IRISDMC', start=UTCDateTime(2014, 5, 1, 0, 0), end=UTCDateTime(2017, 1, 1, 0, 0), site='MC06', latitude=46.552021, longitude=-122.157204, elevation_m=770.0)\n"
     ]
    }
   ],
   "source": [
    "stations = read_csv(\"IMUSH.csv\", start_row=0, end_row=5)\n",
    "\n",
    "for s in stations:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4608341b-7a28-42b8-b6fb-6346eb98c626",
   "metadata": {},
   "source": [
    "Note that the index of the first row of a pandas table or dataframe starts at 0. Note the station in the first row. Change the `start_row` parameter to 0 and compare the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb668b2-99bd-4b13-9780-3936494bb43b",
   "metadata": {},
   "source": [
    "## Downloading MiniSeed files\n",
    "\n",
    "Next we will download a three miniseed file and save it in GeoLab. Using `obspy` we'll write a function to download files by a list we provide. We can use `obspy` to request the data from EarthScope's FDSN service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07f928cf-85b7-44fb-8041-30cec26068dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_miniseed(station_rows, *, starttime=None, endtime=None, output_dir=\"./seismic_data\"):\n",
    "    \"\"\"\n",
    "    Download miniseed file from EarthScope's FDSN service.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    station_rows : str\n",
    "        Station code (e.g., 'ANMO')\n",
    "    start_date : str\n",
    "        Start date in format 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SS'\n",
    "    end_date : str\n",
    "        End date in format 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SS'\n",
    "    output_dir : str, optional\n",
    "        Directory to save the miniseed file (default: './seismic_data')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str : Path to the saved miniseed file\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> download_miniseed('ANMO', 'IU', '2024-01-01', '2024-01-02')\n",
    "    \"\"\"\n",
    "    \n",
    "    # default values\n",
    "    network = \"XD\"\n",
    "    location = \"*\"\n",
    "    channel = \"BH?\"\n",
    "    \n",
    "    # parse list as a tuple\n",
    "    station_data = [(row.station, row.start, row.end) for row in station_rows]\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize EarthScope FDSN client\n",
    "    client = Client('IRIS')  # IRIS is part of EarthScope\n",
    "    \n",
    "    # Download waveform data\n",
    "    for station, start, end in station_data:\n",
    "        actual_start = starttime if starttime is not None else start\n",
    "        actual_end = endtime if endtime is not None else end\n",
    "        starttime=UTCDateTime(actual_start)\n",
    "        endtime=UTCDateTime(actual_end)\n",
    "        \n",
    "        try:\n",
    "            st = client.get_waveforms(\n",
    "                network=network,\n",
    "                station=station,\n",
    "                location=location,\n",
    "                channel=channel,\n",
    "                starttime=starttime,\n",
    "                endtime=endtime\n",
    "            )\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "        # Create filename\n",
    "        filename = f\"{network}_{station}_{starttime.strftime('%Y%m%d')}_{endtime.strftime('%Y%m%d')}.mseed\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Save to miniseed file\n",
    "        st.write(filepath, format='MSEED')\n",
    "        print(f\"Successfully saved to: {filepath}\")\n",
    "        \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0b66a4d-ba07-4ee4-93bf-1871000ac51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved to: ./seismic_data/XD_MA05_20150601_20150630.mseed\n",
      "Successfully saved to: ./seismic_data/XD_MB05_20150601_20150630.mseed\n",
      "Successfully saved to: ./seismic_data/XD_MB07_20150601_20150630.mseed\n",
      "Successfully saved to: ./seismic_data/XD_MC06_20150601_20150630.mseed\n",
      "Successfully saved to: ./seismic_data/XD_MC08_20150601_20150630.mseed\n"
     ]
    }
   ],
   "source": [
    "#---debug---#\n",
    "\n",
    "# import logging\n",
    "\n",
    "# # Configure logging to show HTTP request/response details\n",
    "# try:\n",
    "#     # Python 3\n",
    "#     import http.client as http_client\n",
    "# except ImportError:\n",
    "#     # Python 2\n",
    "#     import httplib as http_client\n",
    "\n",
    "# http_client.HTTPConnection.debuglevel = 1\n",
    "# logging.basicConfig() # you need to initialize logging, otherwise you will not see anything from requests\n",
    "# logging.getLogger().setLevel(logging.DEBUG)\n",
    "# requests_log = logging.getLogger(\"requests.packages.urllib3\")\n",
    "# requests_log.setLevel(logging.DEBUG)\n",
    "# requests_log.propagate = True\n",
    "\n",
    "stations = read_csv(\"IMUSH.csv\", start_row=1, end_row=6)\n",
    "\n",
    "filepath = download_miniseed(stations, starttime=\"2015-06-01T00:00:00\", endtime=\"2015-06-30T00:00:00\")\n",
    "\n",
    "http_client.HTTPConnection.debuglevel = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea7b796-a94d-402b-ac3a-acd8a36827f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
