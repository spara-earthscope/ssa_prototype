{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9fe2d3-3b15-4819-bc96-4ee86863c818",
   "metadata": {},
   "source": [
    "# In-Stream Processing\n",
    "\n",
    "The previous examples demonstrated how to use concurrency to download data, which is useful for working offline or where Internet speeds are limited. However, cloud resources allow for streaming data directly to a function without the overhead of writing the file to disk. Streaming data is more efficient regarding total time to process data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdf2919-76a2-4a0b-83f7-31c58814b6cd",
   "metadata": {},
   "source": [
    "## Streaming Data from Earthscope\n",
    "\n",
    "This notebook demonstrates how to stream seismic miniseed files directly from FDSN servers and process them in parallel WITHOUT downloading first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b61b16-895c-4343-be97-eee26d0f40be",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff6e61b-f5fd-4f05-87ed-72e062f0d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b1ce93-a93c-4147-997d-57f27cec25bd",
   "metadata": {},
   "source": [
    "### Function to Stream and Process a Single Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1954cc28-1add-494b-9e60-2de8ed5703d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_and_process_single_station(station, start, end, network, location, channel,\n",
    "                                     starttime_override, endtime_override, \n",
    "                                     output_dir, client, freqmin, freqmax,\n",
    "                                     taper_percentage, corners, zerophase):\n",
    "    \"\"\"\n",
    "    Stream miniseed data from FDSN server and process it in memory.\n",
    "    \n",
    "    This function combines downloading and processing into a single operation,\n",
    "    streaming data directly from the server without saving raw files first.\n",
    "    \n",
    "    Processing steps:\n",
    "    1. Stream waveform data from FDSN server\n",
    "    2. Linear detrend - removes linear trends in the data\n",
    "    3. Demean - removes the mean value (centers data around zero)\n",
    "    4. Taper - applies a window to reduce edge effects\n",
    "    5. Bandpass filter - keeps only frequencies within specified range\n",
    "    6. Save processed data to output directory\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    station : str\n",
    "        Station code (e.g., 'ANMO')\n",
    "    start : str\n",
    "        Station's start date from metadata\n",
    "    end : str\n",
    "        Station's end date from metadata\n",
    "    network : str\n",
    "        Network code (e.g., 'XD')\n",
    "    location : str\n",
    "        Location code (e.g., '*' for all locations)\n",
    "    channel : str\n",
    "        Channel code (e.g., 'BH?' for all BH channels)\n",
    "    starttime_override : str or None\n",
    "        Override start time if provided\n",
    "    endtime_override : str or None\n",
    "        Override end time if provided\n",
    "    output_dir : str\n",
    "        Directory to save the processed file\n",
    "    client : obspy.clients.fdsn.Client\n",
    "        FDSN client instance for streaming data\n",
    "    freqmin : float\n",
    "        Minimum frequency for bandpass filter (Hz)\n",
    "    freqmax : float\n",
    "        Maximum frequency for bandpass filter (Hz)\n",
    "    taper_percentage : float\n",
    "        Percentage of trace to taper (0-0.5)\n",
    "    corners : int\n",
    "        Number of corners for Butterworth filter\n",
    "    zerophase : bool\n",
    "        If True, apply zero-phase filter\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (success: bool, station: str, output_file: str or None, error: str or None)\n",
    "    \"\"\"\n",
    "    # Determine actual start/end times\n",
    "    actual_start = starttime_override if starttime_override is not None else start\n",
    "    actual_end = endtime_override if endtime_override is not None else end\n",
    "    starttime = UTCDateTime(actual_start)\n",
    "    endtime = UTCDateTime(actual_end)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Stream waveform data directly from FDSN server\n",
    "        # This happens in memory - no file is downloaded to disk\n",
    "        st = client.get_waveforms(\n",
    "            network=network,\n",
    "            station=station,\n",
    "            location=location,\n",
    "            channel=channel,\n",
    "            starttime=starttime,\n",
    "            endtime=endtime\n",
    "        )\n",
    "        \n",
    "        n_traces = len(st)\n",
    "        \n",
    "        # Step 2-5: Process each trace in memory\n",
    "        for tr in st:\n",
    "            # Apply linear detrend\n",
    "            tr.detrend('linear')\n",
    "            \n",
    "            # Remove mean\n",
    "            tr.detrend('demean')\n",
    "            \n",
    "            # Apply taper\n",
    "            tr.taper(max_percentage=taper_percentage, type='hann')\n",
    "            \n",
    "            # Apply bandpass filter\n",
    "            tr.filter('bandpass',\n",
    "                     freqmin=freqmin,\n",
    "                     freqmax=freqmax,\n",
    "                     corners=corners,\n",
    "                     zerophase=zerophase)\n",
    "        \n",
    "        # Step 6: Save processed data to output directory\n",
    "        filename = f\"{network}_{station}_{starttime.strftime('%Y%m%d')}_{endtime.strftime('%Y%m%d')}_processed.mseed\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        st.write(filepath, format='MSEED')\n",
    "        \n",
    "        print(f\"✓ Streamed & processed: {station} ({n_traces} trace(s))\")\n",
    "        return (True, station, filepath, None)\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"{type(e).__name__}: {str(e)}\"\n",
    "        print(f\"✗ Failed: {station} - {error_msg}\")\n",
    "        return (False, station, None, error_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93a132-ee29-4c1a-b520-e0bb089222ac",
   "metadata": {},
   "source": [
    "> **Explainer:**\n",
    "> \n",
    "> This function combines the download and processing steps into a single operation that happens entirely in memory. Instead of downloading raw files first, it streams waveform data directly from the FDSN server using the ObsPy client's get_waveforms() method.\n",
    "> \n",
    "> The data is held in memory as an ObsPy Stream object, then immediately processed with the same four-step workflow (detrend, demean, taper, filter). Only the final processed data is saved to disk, which saves significant disk space and I/O time. This approach is ideal for processing large datasets where storing both raw and processed files would be prohibitive, and it's designed to work in parallel so multiple stations can be streamed and processed simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d68723f-d62e-4645-af27-a1f715ad2e7f",
   "metadata": {},
   "source": [
    "### Main Parallel Streaming and Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad73ff9d-7233-441b-b6b4-18640a3a40a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_and_process_parallel(station_rows, *, starttime=None, endtime=None,\n",
    "                                output_dir=\"./processed_data\",\n",
    "                                freqmin=0.1, freqmax=10.0,\n",
    "                                taper_percentage=0.05, corners=4,\n",
    "                                zerophase=True, max_workers=5,\n",
    "                                network=\"XD\", location=\"*\", channel=\"BH?\",\n",
    "                                fdsn_client=\"IRIS\"):\n",
    "    \"\"\"\n",
    "    Stream and process miniseed data from FDSN servers in parallel.\n",
    "    \n",
    "    This function orchestrates parallel streaming and processing of seismic data\n",
    "    from multiple stations. Data is streamed directly from FDSN servers and\n",
    "    processed in memory without intermediate file storage.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    station_rows : iterable\n",
    "        Iterable of station objects with .station, .start, .end attributes\n",
    "        (e.g., rows from a pandas DataFrame)\n",
    "    starttime : str, optional\n",
    "        Override start time in format 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SS'\n",
    "    endtime : str, optional\n",
    "        Override end time in format 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SS'\n",
    "    output_dir : str, optional\n",
    "        Directory to save processed files (default: './processed_data')\n",
    "    freqmin : float, optional\n",
    "        Minimum frequency for bandpass filter in Hz (default: 0.1)\n",
    "    freqmax : float, optional\n",
    "        Maximum frequency for bandpass filter in Hz (default: 10.0)\n",
    "    taper_percentage : float, optional\n",
    "        Percentage of trace to taper, 0-0.5 (default: 0.05 = 5%)\n",
    "    corners : int, optional\n",
    "        Number of corners for Butterworth filter (default: 4)\n",
    "    zerophase : bool, optional\n",
    "        If True, apply zero-phase filter (default: True)\n",
    "    max_workers : int, optional\n",
    "        Maximum number of parallel workers (default: 5)\n",
    "    network : str, optional\n",
    "        Network code (default: 'XD')\n",
    "    location : str, optional\n",
    "        Location code (default: '*')\n",
    "    channel : str, optional\n",
    "        Channel code (default: 'BH?')\n",
    "    fdsn_client : str, optional\n",
    "        FDSN client name (default: 'IRIS')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing 'successful' and 'failed' processing results\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> stations_df = pd.DataFrame({\n",
    "    ...     'station': ['ANMO', 'CCM', 'HLID'],\n",
    "    ...     'start': ['2024-01-01', '2024-01-01', '2024-01-01'],\n",
    "    ...     'end': ['2024-01-02', '2024-01-02', '2024-01-02']\n",
    "    ... })\n",
    "    >>> results = stream_and_process_parallel(\n",
    "    ...     stations_df.itertuples(),\n",
    "    ...     starttime='2024-01-01',\n",
    "    ...     endtime='2024-01-02',\n",
    "    ...     max_workers=10\n",
    "    ... )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parse station data\n",
    "    station_data = [(row.station, row.start, row.end) for row in station_rows]\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize FDSN client (thread-safe, can be shared across workers)\n",
    "    client = Client(fdsn_client)\n",
    "    \n",
    "    # Print configuration\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Stream & Process Seismic Data (In-Memory)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"FDSN server:      {fdsn_client}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(f\"Stations:         {len(station_data)}\")\n",
    "    print(f\"Parallel workers: {max_workers}\")\n",
    "    print(f\"\\nData parameters:\")\n",
    "    print(f\"  - Network:           {network}\")\n",
    "    print(f\"  - Location:          {location}\")\n",
    "    print(f\"  - Channel:           {channel}\")\n",
    "    if starttime:\n",
    "        print(f\"  - Start time:        {starttime}\")\n",
    "    if endtime:\n",
    "        print(f\"  - End time:          {endtime}\")\n",
    "    print(f\"\\nProcessing parameters:\")\n",
    "    print(f\"  - Linear detrend:    enabled\")\n",
    "    print(f\"  - Demean:            enabled\")\n",
    "    print(f\"  - Taper:             {taper_percentage*100}% (Hann window)\")\n",
    "    print(f\"  - Bandpass filter:   {freqmin}-{freqmax} Hz\")\n",
    "    print(f\"  - Filter corners:    {corners}\")\n",
    "    print(f\"  - Zero-phase:        {zerophase}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Initialize results\n",
    "    results = {\n",
    "        'successful': [],\n",
    "        'failed': []\n",
    "    }\n",
    "    \n",
    "    # Process stations in parallel using ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all streaming and processing tasks\n",
    "        future_to_station = {\n",
    "            executor.submit(\n",
    "                stream_and_process_single_station,\n",
    "                station, start, end, network, location, channel,\n",
    "                starttime, endtime, output_dir, client,\n",
    "                freqmin, freqmax, taper_percentage, corners, zerophase\n",
    "            ): station\n",
    "            for station, start, end in station_data\n",
    "        }\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in as_completed(future_to_station):\n",
    "            station = future_to_station[future]\n",
    "            try:\n",
    "                success, station_name, output_file, error = future.result()\n",
    "                \n",
    "                if success:\n",
    "                    results['successful'].append({\n",
    "                        'station': station_name,\n",
    "                        'output_file': output_file\n",
    "                    })\n",
    "                else:\n",
    "                    results['failed'].append({\n",
    "                        'station': station_name,\n",
    "                        'error': error\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Unexpected error: {str(e)}\"\n",
    "                print(f\"✗ Failed: {station} - {error_msg}\")\n",
    "                results['failed'].append({\n",
    "                    'station': station,\n",
    "                    'error': error_msg\n",
    "                })\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing Summary:\")\n",
    "    print(f\"  ✓ Successful: {len(results['successful'])} stations\")\n",
    "    print(f\"  ✗ Failed:     {len(results['failed'])} stations\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if results['failed']:\n",
    "        print(f\"\\nFailed stations:\")\n",
    "        for item in results['failed']:\n",
    "            print(f\"  - {item['station']}: {item['error']}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276e866-6c7b-4086-87f8-0e994e0f63ef",
   "metadata": {},
   "source": [
    "> **Explainer:**\n",
    "> \n",
    "> This is the main orchestration function that manages the parallel streaming and processing workflow. Unlike traditional approaches that download files first and then process them, this function combines both operations into a single streaming pipeline.\n",
    "> \n",
    "> It initializes a thread-safe FDSN client that's shared across all worker threads, then uses `ThreadPoolExecutor` to simultaneously stream and process data from multiple stations. Each worker thread independently connects to the FDSN server, streams waveform data into memory, processes it with the specified filters, and saves only the processed result - all without creating intermediate raw files.\n",
    ">\n",
    "> This approach offers several advantages:\n",
    "> 1. saves disk space by not storing raw data,\n",
    "> 2. reduces I/O overhead,\n",
    "> 3. allows processing to begin immediately as data arrives, and\n",
    "> 4.  scales efficiently for large datasets.\n",
    ">\n",
    "> The parallel execution means that if you're processing 20 stations with 5 workers, you'll get approximately 5x speedup compared to sequential processing, with the added benefit of reduced storage requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caf4c79-bc19-4dbf-912d-4e035180fdf4",
   "metadata": {},
   "source": [
    "### Usage: Basic streaming and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc5b23d2-41ee-4132-8693-3d3abbd1e64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Stream & Process Seismic Data (In-Memory)\n",
      "======================================================================\n",
      "FDSN server:      IRIS\n",
      "Output directory: ./processed_data\n",
      "Stations:         4\n",
      "Parallel workers: 10\n",
      "\n",
      "Data parameters:\n",
      "  - Network:           XD\n",
      "  - Location:          *\n",
      "  - Channel:           BH?\n",
      "  - Start time:        2024-01-01T00:00:00\n",
      "  - End time:          2024-01-01T12:00:00\n",
      "\n",
      "Processing parameters:\n",
      "  - Linear detrend:    enabled\n",
      "  - Demean:            enabled\n",
      "  - Taper:             5.0% (Hann window)\n",
      "  - Bandpass filter:   0.1-10.0 Hz\n",
      "  - Filter corners:    4\n",
      "  - Zero-phase:        True\n",
      "======================================================================\n",
      "\n",
      "✗ Failed: SRU - FDSNNoDataException: No data available for request.\n",
      "HTTP Status code: 204\n",
      "Detailed response of server:\n",
      "\n",
      "\n",
      "✗ Failed: CCM - FDSNNoDataException: No data available for request.\n",
      "HTTP Status code: 204\n",
      "Detailed response of server:\n",
      "\n",
      "\n",
      "✗ Failed: ANMO - FDSNNoDataException: No data available for request.\n",
      "HTTP Status code: 204\n",
      "Detailed response of server:\n",
      "\n",
      "\n",
      "✗ Failed: HLID - FDSNNoDataException: No data available for request.\n",
      "HTTP Status code: 204\n",
      "Detailed response of server:\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Processing Summary:\n",
      "  ✓ Successful: 0 stations\n",
      "  ✗ Failed:     4 stations\n",
      "======================================================================\n",
      "\n",
      "Failed stations:\n",
      "  - SRU: FDSNNoDataException: No data available for request.\n",
      "HTTP Status code: 204\n",
      "Detailed response of server:\n",
      "\n",
      "\n",
      "  - CCM: FDSNNoDataException: No data available for request.\n",
      "HTTP Status code: 204\n",
      "Detailed response of server:\n",
      "\n",
      "\n",
      "  - ANMO: FDSNNoDataException: No data available for request.\n",
      "HTTP Status code: 204\n",
      "Detailed response of server:\n",
      "\n",
      "\n",
      "  - HLID: FDSNNoDataException: No data available for request.\n",
      "HTTP Status code: 204\n",
      "Detailed response of server:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a list of stations to process\n",
    "stations_df = pd.DataFrame({\n",
    "    'station': ['ANMO', 'CCM', 'HLID', 'SRU'],\n",
    "    'start': ['2024-01-01', '2024-01-01', '2024-01-01', '2024-01-01'],\n",
    "    'end': ['2024-01-02', '2024-01-02', '2024-01-02', '2024-01-02']\n",
    "})\n",
    "\n",
    "# Stream and process with default parameters\n",
    "results = stream_and_process_parallel(\n",
    "    stations_df.itertuples(),\n",
    "    starttime='2024-01-01T00:00:00',\n",
    "    endtime='2024-01-01T12:00:00',  # Process 12 hours of data\n",
    "    output_dir='./processed_data',\n",
    "    max_workers=10  # Process 10 stations simultaneously\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9d73bb-1cb2-4da3-bfc0-0a29af97a6ef",
   "metadata": {},
   "source": [
    "> **Explainer:**\n",
    ">\n",
    "> This shows the simplest usage pattern where you define a list of stations (here using a pandas DataFrame) and call the main function with time boundaries. The function will stream data directly from the IRIS FDSN server for all stations in parallel, process each in memory, and save only the processed results.\n",
    ">\n",
    "> With 10 parallel workers, this can process 10 stations simultaneously, providing significant speedup for large station lists. The 12-hour time window is ideal for event-based studies where you want to capture a specific seismic event without downloading days of continuous data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985199a0-89fc-48fe-8adf-d785da3791b9",
   "metadata": {},
   "source": [
    "### Advanced Example with Custom Parameters\n",
    "\n",
    "High-frequency processing for local events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d701d8-e996-4269-922c-084eaa3814ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process with parameters suitable for local/crustal events\n",
    "results = stream_and_process_parallel(\n",
    "    stations_df.itertuples(),\n",
    "    starttime='2024-01-01',\n",
    "    endtime='2024-01-07',  # One week of data\n",
    "    output_dir='./local_events_processed',\n",
    "    freqmin=5.0,           # Higher frequency for local events\n",
    "    freqmax=10.0,          # Capture higher frequencies\n",
    "    taper_percentage=0.1,  # 10% taper\n",
    "    corners=4,\n",
    "    zerophase=True,\n",
    "    max_workers=8,\n",
    "    network='XD',\n",
    "    channel='*HZ',         # Broadband high-gain channels\n",
    "    fdsn_client='IRIS'\n",
    ")\n",
    "\n",
    "print(f\"\\nProcessed {len(results['successful'])} stations successfully\")\n",
    "print(f\"Failed to process {len(results['failed'])} stations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c31bfb-2144-4d2e-91dd-a13db0ea336c",
   "metadata": {},
   "source": [
    "> **Explainer:**\n",
    "> \n",
    "> This example shows how to customize processing parameters for specific applications. The frequency range (5-10 Hz) is suited for local and crustal events, which have more high-frequency content than teleseismic events. Processing a full week of data demonstrates the efficiency advantage of streaming - you avoid storing 7 days × 4 stations = 28 raw files, instead only keeping the 4 processed files.\n",
    ">\n",
    "> The 10% taper (instead of the default 5%) provides more aggressive edge treatment, useful when you have concerns about boundary effects in the filtered data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1468affc-92c6-41ec-b7e9-f171857367b8",
   "metadata": {},
   "source": [
    "### Process Results and Quality Control\n",
    "\n",
    "Analyzing and exporting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba9873-b7c7-4423-b0a2-9af62334f6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display successful processing\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Successfully Processed Stations:\")\n",
    "print(\"=\"*70)\n",
    "for item in results['successful']:\n",
    "    print(f\"Station: {item['station']}\")\n",
    "    print(f\"  Output: {item['output_file']}\")\n",
    "    print()\n",
    "\n",
    "# Check for failures and investigate\n",
    "if results['failed']:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Failed Stations (Need Attention):\")\n",
    "    print(\"=\"*70)\n",
    "    for item in results['failed']:\n",
    "        print(f\"Station: {item['station']}\")\n",
    "        print(f\"  Error: {item['error']}\")\n",
    "        print()\n",
    "\n",
    "# Save results to CSV for documentation\n",
    "successful_df = pd.DataFrame(results['successful'])\n",
    "failed_df = pd.DataFrame(results['failed'])\n",
    "\n",
    "if not successful_df.empty:\n",
    "    successful_df.to_csv('streaming_successful.csv', index=False)\n",
    "    print(f\"✓ Saved successful results to: streaming_successful.csv\")\n",
    "\n",
    "if not failed_df.empty:\n",
    "    failed_df.to_csv('streaming_failed.csv', index=False)\n",
    "    print(f\"✓ Saved failed results to: streaming_failed.csv\")\n",
    "\n",
    "# Calculate success rate\n",
    "total = len(results['successful']) + len(results['failed'])\n",
    "success_rate = len(results['successful']) / total * 100 if total > 0 else 0\n",
    "print(f\"\\nSuccess rate: {success_rate:.1f}% ({len(results['successful'])}/{total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40691c08-9fa6-4ecf-a570-3a4ddf593347",
   "metadata": {},
   "source": [
    "> **Explainer:**\n",
    "> \n",
    "> This cell shows best practices for quality control and documentation after processing. It displays detailed information about both successful and failed stations, exports results to CSV files for record-keeping, and calculates a success rate metric. This\n",
    "is particularly important when processing large numbers of stations, as it helps identify patterns in failures (e.g., certain stations might consistently fail due to data availability issues or network problems). The CSV files can be used for subsequent analysis or to retry failed stations with different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020aee01-0f18-40ae-a307-4c11c26f57f1",
   "metadata": {},
   "source": [
    "### Streaming vs Traditional Approach\n",
    "\n",
    "**Traditional Approach (Download then Process):**\n",
    "\n",
    "1. Download 20 stations × 10 seconds = 200 seconds\n",
    "2. Process 20 files × 5 seconds = 100 seconds\n",
    "\n",
    "   Total: 300 seconds\n",
    "   Storage: Raw files + Processed files (2x storage)\n",
    "\n",
    "Streaming Approach (This Notebook):\n",
    "   - Stream + Process: 20 stations ÷ 5 workers × 10 seconds = 40 seconds\n",
    "   Total: 40 seconds (~7.5x faster!)\n",
    "   Storage: Only processed files (50% storage)\n",
    "\n",
    "**Key Advantages:**\n",
    "\n",
    "1. *Reduced Storage*: Only processed data is saved, saving 50% disk space\n",
    "2. *Faster Processing*: Download and processing happen simultaneously\n",
    "3. *Better Scalability*: Easier to process hundreds or thousands of stations\n",
    "4. *Simplified Workflow*: No intermediate file management needed\n",
    "5. *Fresh Data*: Always gets the latest data from FDSN server\n",
    "\n",
    "**Optimal Worker Count:**\n",
    "\n",
    "- For network-bound operations: 5-20 workers work well\n",
    "- Too many workers can overwhelm FDSN servers (be respectful!)\n",
    "- Balance between speed and server load\n",
    "- IRIS typically allows ~5-10 concurrent connections per user\n",
    "\n",
    "**When to Use Streaming vs Traditional:**\n",
    "\n",
    "Use Streaming when:\n",
    "- Processing many stations with limited disk space\n",
    "- Need only processed data, not raw data\n",
    "- Working with cloud computing (minimize storage costs)\n",
    "\n",
    "Use Traditional (download first) when:\n",
    "- Need to preserve raw data for multiple processing runs\n",
    "- Working offline or with unreliable network\n",
    "- Need to experiment with different processing parameters\n",
    "- Archiving data for long-term studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda16f4-44b6-4aca-89a6-8a61339296e5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
