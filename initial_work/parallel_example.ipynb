{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef80f62d-c825-47f7-8966-460b059fdfc0",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc96ecd-1c2b-4d00-85f5-27ab8cfc5775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from obspy import UTCDateTime, read as obspy_read\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy.clients.fdsn.header import FDSNNoDataException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da2f8e3-5a5d-4ca6-afb0-17d5b423ed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Defaults / knobs\n",
    "# -----------------------------\n",
    "CHANNEL = os.getenv(\"CHANNEL\", \"BH?,HH?\")\n",
    "LOCATION = os.getenv(\"LOCATION\", \"*\")\n",
    "TIMEOUT_S = float(os.getenv(\"TIMEOUT_S\", \"60\"))\n",
    "DEFAULT_MAX_BYTES = int(os.getenv(\"MAX_BYTES\", str(200 * 1024 * 1024)))  # 200 MB\n",
    "\n",
    "DATACENTER_MAP: Dict[str, str] = {\n",
    "    \"IRISDMC\": \"IRIS\",\n",
    "    \"IRIS\": \"IRIS\",\n",
    "    \"SCEDC\": \"SCEDC\",\n",
    "    \"NCEDC\": \"NCEDC\",\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class StationRow:\n",
    "    row_index: int\n",
    "    station: str\n",
    "    datacenter: str\n",
    "    start: UTCDateTime\n",
    "    end: UTCDateTime\n",
    "    site: str\n",
    "    lat: float\n",
    "    lon: float\n",
    "    elev_m: float\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DayTask:\n",
    "    row: StationRow\n",
    "    provider: str\n",
    "    day_start: UTCDateTime\n",
    "    day_end: UTCDateTime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4806d40c-bdef-4518-b036-fdde4c81757b",
   "metadata": {},
   "source": [
    "parse_date_ymd(s: str) -> UTCDateTime\n",
    "\n",
    "Parses a date string like YYYY-MM-DD into an ObsPy UTCDateTime at midnight UTC. It’s used when reading the input station CSV so that each station’s Start/End dates become time objects the rest of the pipeline can work with. Called by read_station_csv()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52fbe8f-a8da-404a-9b4d-a4bbaca2bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date_ymd(s: str) -> UTCDateTime:\n",
    "    return UTCDateTime(s.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796af4e8-29ee-4c21-83a5-0197e5cec016",
   "metadata": {},
   "source": [
    "parse_iso_time(s: str) -> UTCDateTime\n",
    "\n",
    "Parses an ISO-8601 datetime string (e.g., 2014-05-01T00:00:00Z) into UTCDateTime. It’s used for CLI overrides (--start, --end) so you can override the time range for all stations. Called by main()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4531c825-67e0-4f6c-90b7-8df32c21cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_iso_time(s: str) -> UTCDateTime:\n",
    "    return UTCDateTime(s.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea9a97-cd53-4516-aeeb-9627bcee1e17",
   "metadata": {},
   "source": [
    "provider_for_datacenter(dc: str) -> str\n",
    "\n",
    "Maps the CSV “DataCenter” field (e.g., IRISDMC) to an ObsPy FDSN client shortcut (e.g., IRIS). This ensures the correct remote FDSN service is used for station and dataselect calls. Called by read_station_csv() indirectly via build_tasks(), and also used when creating tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac6ca1-e1f4-4b86-a3d6-38882a3fd0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def provider_for_datacenter(dc: str) -> str:\n",
    "    key = dc.strip().upper()\n",
    "    if key not in DATACENTER_MAP:\n",
    "        raise ValueError(f\"Unknown DataCenter '{dc}'. Add it to DATACENTER_MAP.\")\n",
    "    return DATACENTER_MAP[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6c1bd5-0994-41b4-929f-4e142fb4c1b8",
   "metadata": {},
   "source": [
    "read_station_csv(csv_path: Path) -> List[StationRow]\n",
    "\n",
    "Reads the input CSV into a list of StationRow objects (one per row). It validates required columns, converts Start/End to UTCDateTime, and makes the End date inclusive by adding 86400 seconds (so the range covers the full end day). Called by main() to ingest the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26862455-2e35-454c-add6-a2f15dec42a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_station_csv(csv_path: Path) -> List[StationRow]:\n",
    "    rows: List[StationRow] = []\n",
    "    with csv_path.open(\"r\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        required = {\"Station\", \"DataCenter\", \"Start\", \"End\", \"Site\", \"Latitude\", \"Longitude\", \"Elevation\"}\n",
    "        missing = required - set(reader.fieldnames or [])\n",
    "        if missing:\n",
    "            raise ValueError(f\"CSV missing required columns: {sorted(missing)}\")\n",
    "\n",
    "        for i, r in enumerate(reader, start=1):\n",
    "            if not any((v or \"\").strip() for v in r.values()):\n",
    "                continue\n",
    "            start = parse_date_ymd(r[\"Start\"])\n",
    "            end = parse_date_ymd(r[\"End\"]) + 86400  # inclusive end date\n",
    "            rows.append(\n",
    "                StationRow(\n",
    "                    row_index=i,\n",
    "                    station=r[\"Station\"].strip(),\n",
    "                    datacenter=r[\"DataCenter\"].strip(),\n",
    "                    start=start,\n",
    "                    end=end,\n",
    "                    site=(r.get(\"Site\") or \"\").strip(),\n",
    "                    lat=float(r[\"Latitude\"]),\n",
    "                    lon=float(r[\"Longitude\"]),\n",
    "                    elev_m=float(r[\"Elevation\"]),\n",
    "                )\n",
    "            )\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd73c44-ca8f-43eb-830f-93eefbf410bc",
   "metadata": {},
   "source": [
    "iter_days(t1: UTCDateTime, t2: UTCDateTime)\n",
    "\n",
    "Breaks a time interval into daily windows aligned to UTC midnight: yields (day_start, day_end) for each day intersecting [t1, t2). It’s what drives the “one metadata row per day” behavior. Called by build_tasks()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b36bd-62b0-4c02-8bef-ab623eb78b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_days(t1: UTCDateTime, t2: UTCDateTime) -> Iterable[Tuple[UTCDateTime, UTCDateTime]]:\n",
    "    \"\"\"\n",
    "    Yield [day_start, day_end) windows aligned to midnight UTC, intersected with [t1,t2).\n",
    "    \"\"\"\n",
    "    cur = UTCDateTime(t1.date)  # midnight\n",
    "    while cur < t2:\n",
    "        nxt = cur + 86400\n",
    "        win_start = max(cur, t1)\n",
    "        win_end = min(nxt, t2)\n",
    "        if win_end > win_start:\n",
    "            yield (win_start, win_end)\n",
    "        cur = nxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4553f6-1383-4c1d-a13a-bc18dc9e3186",
   "metadata": {},
   "source": [
    "pick_station_channels(inv) -> List[(net, sta, loc, cha)]\n",
    "\n",
    "Extracts unique channel tuples from an ObsPy Inventory (returned by get_stations(level=\"channel\")). The output is used to build a bulk dataselect request. Called by process_day_task()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb569da2-4c1b-498b-97ee-b0efda197ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_station_channels(inv) -> List[tuple[str, str, str, str]]:\n",
    "    out = set()\n",
    "    for net in inv:\n",
    "        for sta in net:\n",
    "            for cha in sta:\n",
    "                loc = cha.location_code or \"\"\n",
    "                out.add((net.code, sta.code, loc, cha.code))\n",
    "    return sorted(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600c7ce0-a044-4a40-b6ec-fb4362e74f09",
   "metadata": {},
   "source": [
    "build_bulk_lines(chan_tuples, t1, t2) -> str\n",
    "\n",
    "Creates the body of an FDSN bulk dataselect request: one line per channel tuple with NET STA LOC CHA START END. It converts blank locations to -- (required by the bulk format). Called by process_day_task() right before streaming waveforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a0057-be7d-4c28-9701-7556825e03c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bulk_lines(chan_tuples: List[tuple[str, str, str, str]], t1: UTCDateTime, t2: UTCDateTime) -> str:\n",
    "    lines = []\n",
    "    for net, sta, loc, cha in chan_tuples:\n",
    "        loc_out = loc if loc != \"\" else \"--\"\n",
    "        lines.append(f\"{net} {sta} {loc_out} {cha} {t1.isoformat()} {t2.isoformat()}\")\n",
    "    return \"\\n\".join(lines) + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc3ef6-b876-41e6-9526-71bce0c90e08",
   "metadata": {},
   "source": [
    "stream_dataselect_bulk(dataselect_base_url, bulk_text, ...) -> bytes\n",
    "\n",
    "Performs an HTTP POST to dataselect /query and streams the response into memory (up to max_bytes). Returns raw MiniSEED bytes for that day/task, or b\"\" if no data. Called by process_day_task() (wrapped by retry logic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cdc85c-44b6-4a07-9695-58b68095dc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_dataselect_bulk(dataselect_base_url: str, bulk_text: str, *, timeout_s: float, max_bytes: int) -> bytes:\n",
    "    url = dataselect_base_url.rstrip(\"/\") + \"/query\"\n",
    "    headers = {\"Content-Type\": \"text/plain\", \"Accept\": \"application/vnd.fdsn.mseed\"}\n",
    "\n",
    "    with requests.post(url, data=bulk_text.encode(\"utf-8\"), headers=headers, stream=True, timeout=timeout_s) as r:\n",
    "        if r.status_code in (204, 404):\n",
    "            return b\"\"\n",
    "        r.raise_for_status()\n",
    "\n",
    "        buf = bytearray()\n",
    "        for chunk in r.iter_content(chunk_size=1024 * 128):\n",
    "            if not chunk:\n",
    "                continue\n",
    "            buf.extend(chunk)\n",
    "            if len(buf) > max_bytes:\n",
    "                raise MemoryError(\n",
    "                    f\"MiniSEED exceeds max_bytes={max_bytes}. Reduce date span per request or increase --max-bytes.\"\n",
    "                )\n",
    "        return bytes(buf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f6b969-99f4-4519-acb7-ff7243113347",
   "metadata": {},
   "source": [
    "is_transient_network_error(e: Exception) -> bool\n",
    "\n",
    "Heuristically classifies errors like connection resets/timeouts as “transient” (worth retrying). It exists to handle network issues and the ObsPy “splitlines” edge-case by retrying instead of failing immediately. Called by with_retries()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a643f-2f93-455c-a459-863797aa2c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_transient_network_error(e: Exception) -> bool:\n",
    "    msg = str(e).lower()\n",
    "    return any(\n",
    "        s in msg\n",
    "        for s in (\n",
    "            \"connection reset\",\n",
    "            \"connection aborted\",\n",
    "            \"remote disconnected\",\n",
    "            \"temporarily unavailable\",\n",
    "            \"timed out\",\n",
    "            \"timeout\",\n",
    "            \"ssl\",\n",
    "            \"chunkedencodingerror\",\n",
    "            \"protocolerror\",\n",
    "            \"max retries exceeded\",\n",
    "        )\n",
    "    ) or isinstance(e, (ConnectionResetError, TimeoutError))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f4702a-88f6-423c-a90a-8f74b3aa4a01",
   "metadata": {},
   "source": [
    "with_retries(fn, retries, backoff)\n",
    "\n",
    "Runs a function and retries it on transient network errors using exponential backoff + jitter. It centralizes “be resilient against resets/timeouts.” Called by process_day_task() around get_stations() and stream_dataselect_bulk()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604d5735-6433-43b7-a7a2-3f16b0392813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_retries(fn, *, retries: int, backoff: float):\n",
    "    last = None\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            return fn()\n",
    "        except FDSNNoDataException:\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "            if attempt == retries or not is_transient_network_error(e):\n",
    "                raise\n",
    "            sleep_s = backoff * (2 ** (attempt - 1)) + random.uniform(0, backoff)\n",
    "            time.sleep(sleep_s)\n",
    "    raise RuntimeError(f\"failed after {retries} retries: {last}\") from last"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b687fd-bf8b-4719-8f2c-714371f42711",
   "metadata": {},
   "source": [
    "compute_trace_data_stats(tr) -> dict\n",
    "\n",
    "Computes numeric stats from the trace’s actual samples: min, max, mean, std, RMS, peak-to-peak, and fraction of finite samples. Converts masked arrays to NaNs and uses float64 for stability. Called by first_trace_daily_row()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e317204e-2186-4369-9993-ac68d76b2570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trace_data_stats(tr) -> dict:\n",
    "    data = tr.data\n",
    "    if np.ma.isMaskedArray(data):\n",
    "        data = data.filled(np.nan)\n",
    "    x = np.asarray(data, dtype=np.float64)\n",
    "\n",
    "    finite = np.isfinite(x)\n",
    "    if not np.any(finite):\n",
    "        return {\n",
    "            \"data_min\": \"\",\n",
    "            \"data_max\": \"\",\n",
    "            \"data_mean\": \"\",\n",
    "            \"data_std\": \"\",\n",
    "            \"data_rms\": \"\",\n",
    "            \"data_p2p\": \"\",\n",
    "            \"finite_frac\": 0.0,\n",
    "        }\n",
    "\n",
    "    xf = x[finite]\n",
    "    mn = float(np.min(xf))\n",
    "    mx = float(np.max(xf))\n",
    "    mean = float(np.mean(xf))\n",
    "    std = float(np.std(xf))\n",
    "    rms = float(np.sqrt(np.mean(xf * xf)))\n",
    "    p2p = float(mx - mn)\n",
    "    finite_frac = float(np.sum(finite) / x.size)\n",
    "\n",
    "    return {\n",
    "        \"data_min\": mn,\n",
    "        \"data_max\": mx,\n",
    "        \"data_mean\": mean,\n",
    "        \"data_std\": std,\n",
    "        \"data_rms\": rms,\n",
    "        \"data_p2p\": p2p,\n",
    "        \"finite_frac\": finite_frac,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1f935b-903d-4c8a-9c31-cf95290dd762",
   "metadata": {},
   "source": [
    "first_trace_daily_row(task: DayTask, tr) -> dict\n",
    "\n",
    "Builds the final output row (a dict) for the CSV: station context, day/time window, trace header info (network/station/channel/npts/etc.), plus computed stats from compute_trace_data_stats(). Called by process_day_task() after parsing and selecting the first trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3b619-3db4-47ec-ab67-e0065f5366bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_trace_daily_row(task: DayTask, tr) -> dict:\n",
    "    row = task.row\n",
    "    stats = tr.stats\n",
    "\n",
    "    out = {\n",
    "        \"csv_row\": row.row_index,\n",
    "        \"csv_station\": row.station,\n",
    "        \"csv_datacenter\": row.datacenter,\n",
    "        \"provider\": task.provider,\n",
    "        \"csv_site\": row.site,\n",
    "        \"csv_latitude\": row.lat,\n",
    "        \"csv_longitude\": row.lon,\n",
    "        \"csv_elevation_m\": row.elev_m,\n",
    "        \"day\": task.day_start.date.strftime(\"%Y-%m-%d\"),\n",
    "        \"request_start\": task.day_start.isoformat(),\n",
    "        \"request_end\": task.day_end.isoformat(),\n",
    "        \"trace_id\": tr.id,\n",
    "        \"network\": getattr(stats, \"network\", \"\"),\n",
    "        \"station\": getattr(stats, \"station\", \"\"),\n",
    "        \"location\": getattr(stats, \"location\", \"\"),\n",
    "        \"channel\": getattr(stats, \"channel\", \"\"),\n",
    "        \"sampling_rate\": getattr(stats, \"sampling_rate\", \"\"),\n",
    "        \"npts\": getattr(stats, \"npts\", \"\"),\n",
    "        \"starttime\": stats.starttime.isoformat() if getattr(stats, \"starttime\", None) else \"\",\n",
    "        \"endtime\": stats.endtime.isoformat() if getattr(stats, \"endtime\", None) else \"\",\n",
    "        \"delta\": getattr(stats, \"delta\", \"\"),\n",
    "        \"calib\": getattr(stats, \"calib\", \"\"),\n",
    "    }\n",
    "    out.update(compute_trace_data_stats(tr))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db72b338-2dca-47bd-aba4-bd55004fa80e",
   "metadata": {},
   "source": [
    "process_day_task(task: DayTask, ...) -> Optional[dict]\n",
    "\n",
    "This is the core per-day worker. For one (station, day) task, it: creates an ObsPy client, gets station channels for that day, builds a bulk request, streams MiniSEED bytes, parses them into an ObsPy Stream, selects the first trace, computes stats, and returns a row dict. If there’s no data, returns None. Called by main() via ThreadPoolExecutor.submit()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5041f09-c73a-4b4c-97c3-329b3d6abbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_day_task(task: DayTask, *, max_bytes: int, retries: int, backoff: float) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Worker: station discovery + streaming dataselect + parse + first-trace stats.\n",
    "    Returns a CSV row dict or None if no data.\n",
    "    \"\"\"\n",
    "    # Per-worker clients to avoid shared state\n",
    "    client = Client(task.provider, timeout=TIMEOUT_S)\n",
    "    dataselect_base = client.base_url.rstrip(\"/\") + \"/fdsnws/dataselect/1\"\n",
    "\n",
    "    # 1) station discovery for this day\n",
    "    try:\n",
    "        inv = with_retries(\n",
    "            lambda: client.get_stations(\n",
    "                network=\"*\",\n",
    "                station=task.row.station,\n",
    "                location=LOCATION,\n",
    "                channel=CHANNEL,\n",
    "                starttime=task.day_start,\n",
    "                endtime=task.day_end,\n",
    "                level=\"channel\",\n",
    "                includerestricted=False,\n",
    "            ),\n",
    "            retries=retries,\n",
    "            backoff=backoff,\n",
    "        )\n",
    "    except FDSNNoDataException:\n",
    "        return None\n",
    "\n",
    "    chan_tuples = pick_station_channels(inv)\n",
    "    if not chan_tuples:\n",
    "        return None\n",
    "\n",
    "    bulk_text = build_bulk_lines(chan_tuples, task.day_start, task.day_end)\n",
    "\n",
    "    # 2) stream MiniSEED bytes\n",
    "    try:\n",
    "        mseed_bytes = with_retries(\n",
    "            lambda: stream_dataselect_bulk(\n",
    "                dataselect_base,\n",
    "                bulk_text,\n",
    "                timeout_s=TIMEOUT_S,\n",
    "                max_bytes=max_bytes,\n",
    "            ),\n",
    "            retries=retries,\n",
    "            backoff=backoff,\n",
    "        )\n",
    "    except FDSNNoDataException:\n",
    "        return None\n",
    "    except Exception:\n",
    "        # let caller log the specific error\n",
    "        raise\n",
    "\n",
    "    if not mseed_bytes:\n",
    "        return None\n",
    "\n",
    "    # 3) parse and take first trace\n",
    "    st = obspy_read(io.BytesIO(mseed_bytes))\n",
    "    if len(st) == 0:\n",
    "        return None\n",
    "\n",
    "    return first_trace_daily_row(task, st[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbb0937-4b8c-4d8c-b383-3cc921d107c0",
   "metadata": {},
   "source": [
    "build_tasks(rows, override_start, override_end) -> List[DayTask]\n",
    "\n",
    "Creates a list of tasks—one per day per station—using iter_days() and provider_for_datacenter(). This is what allows parallelization at the “day chunk” level. Called by main()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb6bcc-5990-4ea1-b542-2c6aa9d99ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tasks(rows: List[StationRow], override_start: Optional[UTCDateTime], override_end: Optional[UTCDateTime]) -> List[DayTask]:\n",
    "    tasks: List[DayTask] = []\n",
    "    for r in rows:\n",
    "        provider = provider_for_datacenter(r.datacenter)\n",
    "        start = override_start if override_start is not None else r.start\n",
    "        end = override_end if override_end is not None else r.end\n",
    "        if end <= start:\n",
    "            continue\n",
    "        for d1, d2 in iter_days(start, end):\n",
    "            tasks.append(DayTask(row=r, provider=provider, day_start=d1, day_end=d2))\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2911f6-ca0c-4124-b385-bd6a234a3ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_argparser() -> argparse.ArgumentParser\n",
    "\n",
    "Defines CLI arguments (CSV path, output dir, overrides, worker count, retry/backoff, etc.). Called by main() to parse arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ae797-6ee6-46b4-b179-a4ff5eaf9c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_argparser() -> argparse.ArgumentParser:\n",
    "    p = argparse.ArgumentParser(description=\"Parallel daily streaming MiniSEED -> first trace stats -> CSV.\")\n",
    "    p.add_argument(\"csv_path\", help=\"Input station CSV\")\n",
    "    p.add_argument(\"output_dir\", help=\"Directory to write output CSV\")\n",
    "    p.add_argument(\"--start\", default=None, help=\"Override start time for ALL stations (ISO-8601 UTC).\")\n",
    "    p.add_argument(\"--end\", default=None, help=\"Override end time for ALL stations (ISO-8601 UTC).\")\n",
    "    p.add_argument(\"--workers\", type=int, default=min(6, (os.cpu_count() or 4) * 2), help=\"Parallel worker threads.\")\n",
    "    p.add_argument(\"--max-bytes\", type=int, default=DEFAULT_MAX_BYTES, help=\"Max bytes to buffer per day request.\")\n",
    "    p.add_argument(\"--retries\", type=int, default=6, help=\"Retries for transient network errors.\")\n",
    "    p.add_argument(\"--backoff\", type=float, default=0.8, help=\"Base backoff seconds for retries.\")\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1900f3-cc37-48e5-a90e-77a37928eaf1",
   "metadata": {},
   "source": [
    "main() is the program orchestrator. It parses CLI args, reads the station CSV into structured rows, applies optional global start/end overrides, expands those rows into per-day tasks, and then runs those tasks concurrently with a ThreadPoolExecutor. Each worker returns either a CSV row dict (success) or None (no data), while errors are caught and logged. Importantly, only the main thread writes to the CSV (to avoid corrupting output), so workers return results and main() appends them to disk as futures complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b049cc2f-1a9b-4349-aef2-71dbaa4d760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    args = build_argparser().parse_args()\n",
    "\n",
    "    csv_path = Path(args.csv_path).expanduser().resolve()\n",
    "    out_dir = Path(args.output_dir).expanduser().resolve()\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    rows = read_station_csv(csv_path)\n",
    "    if not rows:\n",
    "        print(\"No stations found in CSV.\", file=sys.stderr)\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    override_start = parse_iso_time(args.start) if args.start else None\n",
    "    override_end = parse_iso_time(args.end) if args.end else None\n",
    "\n",
    "    tasks = build_tasks(rows, override_start, override_end)\n",
    "    if not tasks:\n",
    "        print(\"No day tasks to process (check time window).\", file=sys.stderr)\n",
    "        raise SystemExit(1)\n",
    "\n",
    "    out_csv = out_dir / \"daily_first_trace_stats.csv\"\n",
    "    fieldnames = [\n",
    "        \"csv_row\",\n",
    "        \"csv_station\",\n",
    "        \"csv_datacenter\",\n",
    "        \"provider\",\n",
    "        \"csv_site\",\n",
    "        \"csv_latitude\",\n",
    "        \"csv_longitude\",\n",
    "        \"csv_elevation_m\",\n",
    "        \"day\",\n",
    "        \"request_start\",\n",
    "        \"request_end\",\n",
    "        \"trace_id\",\n",
    "        \"network\",\n",
    "        \"station\",\n",
    "        \"location\",\n",
    "        \"channel\",\n",
    "        \"sampling_rate\",\n",
    "        \"npts\",\n",
    "        \"starttime\",\n",
    "        \"endtime\",\n",
    "        \"delta\",\n",
    "        \"calib\",\n",
    "        \"data_min\",\n",
    "        \"data_max\",\n",
    "        \"data_mean\",\n",
    "        \"data_std\",\n",
    "        \"data_rms\",\n",
    "        \"data_p2p\",\n",
    "        \"finite_frac\",\n",
    "    ]\n",
    "\n",
    "    print(\n",
    "        f\"Stations={len(rows)}  DayTasks={len(tasks)}  workers={args.workers}  \"\n",
    "        f\"CHANNEL={CHANNEL}  LOCATION={LOCATION}  max_bytes={args.max_bytes}  retries={args.retries}\"\n",
    "    )\n",
    "    if override_start or override_end:\n",
    "        print(f\"Override window: start={override_start} end={override_end}\")\n",
    "    print(f\"Writing CSV: {out_csv}\")\n",
    "\n",
    "    wrote_header = False\n",
    "    ok = skipped = err = 0\n",
    "\n",
    "    # main thread is the ONLY writer\n",
    "    with ThreadPoolExecutor(max_workers=args.workers) as ex:\n",
    "        futs = [\n",
    "            ex.submit(process_day_task, t, max_bytes=args.max_bytes, retries=args.retries, backoff=args.backoff)\n",
    "            for t in tasks\n",
    "        ]\n",
    "\n",
    "        for fut in as_completed(futs):\n",
    "            try:\n",
    "                rowdict = fut.result()\n",
    "            except Exception as e:\n",
    "                err += 1\n",
    "                print(f\"ERR  task failed: {e}\")\n",
    "                continue\n",
    "\n",
    "            if rowdict is None:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            with out_csv.open(\"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "                w = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                if not wrote_header:\n",
    "                    w.writeheader()\n",
    "                    wrote_header = True\n",
    "                w.writerow(rowdict)\n",
    "\n",
    "            ok += 1\n",
    "            print(\n",
    "                f\"OK   {rowdict['csv_station']} {rowdict['day']} {rowdict['trace_id']} \"\n",
    "                f\"(npts={rowdict['npts']})\"\n",
    "            )\n",
    "\n",
    "    print(f\"\\nDone. ok={ok} skipped={skipped} error={err}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
